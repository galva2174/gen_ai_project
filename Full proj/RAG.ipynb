{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":117693,"status":"ok","timestamp":1742316097767,"user":{"displayName":"Mohul YP","userId":"16224925862190581899"},"user_tz":-330},"id":"6uR_xiRmcG_6","outputId":"c34e2bab-ebd4-4a84-a605-af9f8556c4fa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pinecone\n","  Downloading pinecone-6.0.2-py3-none-any.whl.metadata (9.0 kB)\n","Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone) (2025.1.31)\n","Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone)\n","  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n","Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from pinecone) (2.8.2)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.11/dist-packages (from pinecone) (4.12.2)\n","Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone) (2.3.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n","Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.28.1)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n","Downloading pinecone-6.0.2-py3-none-any.whl (421 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m421.9/421.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n","Installing collected packages: pinecone-plugin-interface, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, pinecone, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pinecone-6.0.2 pinecone-plugin-interface-0.0.7\n"]}],"source":["!pip install pinecone sentence-transformers transformers torch"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YKamFP6lbvE1","executionInfo":{"status":"ok","timestamp":1742318624457,"user_tz":-330,"elapsed":7298,"user":{"displayName":"Mohul YP","userId":"16224925862190581899"}},"outputId":"458f2dc9-0324-40b1-f8b5-759cbee45cb7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Initializing RAG system...\n","Using device: cpu\n","Processing query: 'What is machine learning?'\n","Generating embedding for question: What is machine learning?\n","Retrieving relevant chunks from Pinecone...\n"]},{"output_type":"stream","name":"stderr","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (667 > 512). Running this sequence through the model will result in indexing errors\n"]},{"output_type":"stream","name":"stdout","text":["Generating answer...\n","\n","==================================================\n","QUESTION:\n","What is machine learning?\n","\n","ANSWER:\n","a sub domain of computer science\n","\n","SOURCES:\n","1. Score: 0.63, Text: according to Wikipedia machine learning is a field of study in artificial intelligence concerned wit...\n","2. Score: 0.60, Text: we know humans learn from their past experiences and machines follow instructions given by humans bu...\n","3. Score: 0.60, Text: a small example in one of the many machine learning algorithms quite easy right believe me it is but...\n","4. Score: 0.59, Text: label, and I know the true label here is G. So this is this is actually supervised learning. All rig...\n","5. Score: 0.57, Text: computers are really, really good at understanding math, right at understanding numbers, they're not...\n","==================================================\n"]}],"source":["  import os\n","  import pinecone\n","  from sentence_transformers import SentenceTransformer\n","  from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","  import torch\n","  from typing import List, Dict, Any\n","\n","  class RAGSystem:\n","      def __init__(self, pinecone_api_key: str, index_name: str, model_name: str = \"sentence-transformers/all-mpnet-base-v2\",\n","                  llm_model_name: str = \"meta-llama/Llama-2-7b-chat-hf\"):\n","          \"\"\"\n","          Initialize RAG system with Pinecone and models.\n","\n","          Args:\n","              pinecone_api_key: Your Pinecone API key\n","              index_name: Name of the Pinecone index to query\n","              model_name: Name of the SentenceTransformer model for embeddings\n","              llm_model_name: Name of the language model for answer generation\n","          \"\"\"\n","          self.tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n","          # Initialize Pinecone client\n","          self.pc = pinecone.Pinecone(api_key=pinecone_api_key)\n","          self.index = self.pc.Index(index_name)\n","\n","          # Initialize embedding model\n","          self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","          print(f\"Using device: {self.device}\")\n","          self.embedding_model = SentenceTransformer(model_name).to(self.device)\n","\n","          self.llm = AutoModelForSeq2SeqLM.from_pretrained(\n","            llm_model_name,\n","            torch_dtype=torch.float16 if self.device == 'cuda' else torch.float32,\n","            device_map=\"auto\"\n","          )\n","\n","          # Set parameters\n","          self.top_k = 5  # Number of chunks to retrieve\n","\n","      def embed_query(self, query: str) -> List[float]:\n","          \"\"\"\n","          Generate embedding for the query using SentenceTransformer.\n","\n","          Args:\n","              query: The user's question\n","\n","          Returns:\n","              Vector embedding of the query\n","          \"\"\"\n","          return self.embedding_model.encode(query).tolist()\n","\n","      def retrieve_relevant_chunks(self, query_embedding: List[float], top_k: int = 5) -> List[Dict[str, Any]]:\n","          \"\"\"\n","          Retrieve the most relevant chunks from Pinecone based on the query embedding.\n","\n","          Args:\n","              query_embedding: The embedding of the user's question\n","              top_k: Number of most relevant chunks to retrieve\n","\n","          Returns:\n","              List of relevant chunks with their metadata\n","          \"\"\"\n","          # Query Pinecone index\n","          query_response = self.index.query(\n","              vector=query_embedding,\n","              top_k=top_k,\n","              include_metadata=True\n","          )\n","\n","          # Extract matches with their metadata\n","          matches = query_response['matches']\n","\n","          # Format results\n","          results = []\n","          for match in matches:\n","              # Extract text sample from metadata if available\n","              text = match.metadata.get('text_sample', 'No text available')\n","\n","              # Format entities if available\n","              entities = match.metadata.get('chunk_entities', '[]')\n","\n","              # Format the result\n","              result = {\n","                  'id': match.id,\n","                  'score': match.score,\n","                  'text': text,\n","                  'entities': entities,\n","                  'video_id': match.metadata.get('video_id', 'unknown'),\n","                  'chunk_id': match.metadata.get('chunk_id', -1)\n","              }\n","              results.append(result)\n","\n","          return results\n","\n","      def generate_answer(self, query: str, relevant_chunks: List[Dict[str, Any]]) -> str:\n","          \"\"\"\n","          Generate an answer using the language model based on the query and relevant chunks.\n","\n","          Args:\n","              query: The user's question\n","              relevant_chunks: List of relevant chunks retrieved from Pinecone\n","\n","          Returns:\n","              Generated answer\n","          \"\"\"\n","          # Prepare the context by combining the relevant chunks\n","          context = \"\\n\\n\".join([f\"Chunk {i+1} (Score: {chunk['score']:.2f}): {chunk['text']}\"\n","                              for i, chunk in enumerate(relevant_chunks)])\n","\n","          # Prepare the prompt for the language model\n","          prompt = f\"\"\"\n","          You are a helpful AI assistant. Use the following context from video transcripts to answer the user's question.\n","\n","          CONTEXT:\n","          {context}\n","\n","          USER QUESTION:\n","          {query}\n","\n","          Answer the question based on the provided context. If the context doesn't contain enough information to answer the question fully, acknowledge that and provide the best possible answer with the available information.\n","\n","          ANSWER:\n","          \"\"\"\n","\n","          # Tokenize the prompt\n","          inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n","\n","          # Generate a response\n","          with torch.no_grad():\n","              outputs = self.llm.generate(\n","                  inputs[\"input_ids\"],\n","                  max_length=2048,\n","                  temperature=0.7,\n","                  top_p=0.9,\n","                  num_return_sequences=1\n","              )\n","\n","          # Decode the response\n","          response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","          # Extract the answer part from the response\n","          answer = response.split(\"ANSWER:\")[1].strip() if \"ANSWER:\" in response else response\n","\n","          return answer\n","\n","      def query(self, question: str) -> Dict[str, Any]:\n","          \"\"\"\n","          Process a user query and return an answer with supporting evidence.\n","\n","          Args:\n","              question: The user's question\n","\n","          Returns:\n","              Dictionary containing the answer and supporting evidence\n","          \"\"\"\n","          # Step 1: Generate embedding for the question\n","          print(f\"Generating embedding for question: {question}\")\n","          query_embedding = self.embed_query(question)\n","\n","          # Step 2: Retrieve relevant chunks from Pinecone\n","          print(f\"Retrieving relevant chunks from Pinecone...\")\n","          relevant_chunks = self.retrieve_relevant_chunks(query_embedding, self.top_k)\n","\n","          # Step 3: Generate an answer based on the retrieved chunks\n","          print(f\"Generating answer...\")\n","          answer = self.generate_answer(question, relevant_chunks)\n","\n","          # Step 4: Return the answer and supporting evidence\n","          result = {\n","              \"question\": question,\n","              \"answer\": answer,\n","              \"sources\": relevant_chunks\n","          }\n","\n","          return result\n","\n","\n","  # Example usage\n","  def main():\n","      # Initialize the RAG system\n","      pinecone_api_key = \"pcsk_7EKroD_MaZi2zjikyZTdpaDPCkit4qEAE6cjKuJ7C2ot9htS7EE6uurWQLrfznykMd7bW3\"\n","      index_name = \"embeddings\"\n","\n","      print(\"Initializing RAG system...\")\n","      rag = RAGSystem(\n","          pinecone_api_key=pinecone_api_key,\n","          index_name=index_name,\n","          # For demo purposes, we can use a smaller model - replace with your preferred model\n","          llm_model_name=\"google/flan-t5-base\"  # Smaller model for demonstration\n","      )\n","\n","      # Example query\n","      question = \"What is machine learning?\"\n","\n","      print(f\"Processing query: '{question}'\")\n","      result = rag.query(question)\n","\n","      # Print the result\n","      print(\"\\n\" + \"=\"*50)\n","      print(\"QUESTION:\")\n","      print(question)\n","      print(\"\\nANSWER:\")\n","      print(result[\"answer\"])\n","      print(\"\\nSOURCES:\")\n","      for i, source in enumerate(result[\"sources\"]):\n","          print(f\"{i+1}. Score: {source['score']:.2f}, Text: {source['text'][:100]}...\")\n","      print(\"=\"*50)\n","\n","  if __name__ == \"__main__\":\n","      main()"]},{"cell_type":"code","source":[],"metadata":{"id":"HRzeyPGJLBZU"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"toc_visible":true,"provenance":[],"authorship_tag":"ABX9TyPje6A1liVHWonh7HD0qpMI"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}